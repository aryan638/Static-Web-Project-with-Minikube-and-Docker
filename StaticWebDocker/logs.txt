
==> Audit <==
|------------|--------------------------|----------|------------------------------|---------|---------------------|---------------------|
|  Command   |           Args           | Profile  |             User             | Version |     Start Time      |      End Time       |
|------------|--------------------------|----------|------------------------------|---------|---------------------|---------------------|
| start      |                          | minikube | AzureAD\AryanSriharshaChippa | v1.35.0 | 04 Feb 25 12:09 IST | 04 Feb 25 12:13 IST |
| docker-env | minikube docker-env      | minikube | AzureAD\AryanSriharshaChippa | v1.35.0 | 04 Feb 25 12:15 IST | 04 Feb 25 12:15 IST |
| docker-env | minikube docker-env      | minikube | AzureAD\AryanSriharshaChippa | v1.35.0 | 04 Feb 25 12:16 IST | 04 Feb 25 12:16 IST |
| service    | static-web-service --url | minikube | AzureAD\AryanSriharshaChippa | v1.35.0 | 04 Feb 25 12:25 IST |                     |
|------------|--------------------------|----------|------------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/04 12:09:38
Running on machine: L200621-7013
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0204 12:09:38.301545    3860 out.go:345] Setting OutFile to fd 84 ...
I0204 12:09:38.302546    3860 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0204 12:09:38.302546    3860 out.go:358] Setting ErrFile to fd 88...
I0204 12:09:38.302546    3860 out.go:392] TERM=,COLORTERM=, which probably does not support color
W0204 12:09:38.313480    3860 root.go:314] Error reading config file at C:\Users\AryanSriharshaChippa\.minikube\config\config.json: open C:\Users\AryanSriharshaChippa\.minikube\config\config.json: The system cannot find the path specified.
I0204 12:09:40.602617    3860 out.go:352] Setting JSON to false
I0204 12:09:40.606345    3860 start.go:129] hostinfo: {"hostname":"L200621-7013","uptime":194,"bootTime":1738650986,"procs":262,"os":"windows","platform":"Microsoft Windows 10 Enterprise","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5371 Build 19045.5371","kernelVersion":"10.0.19045.5371 Build 19045.5371","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"5b1fde55-5177-44be-91a9-d5c376aef662"}
W0204 12:09:40.606345    3860 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0204 12:09:40.649498    3860 out.go:177] * minikube v1.35.0 on Microsoft Windows 10 Enterprise 10.0.19045.5371 Build 19045.5371
I0204 12:09:40.651018    3860 notify.go:220] Checking for updates...
W0204 12:09:40.651018    3860 preload.go:293] Failed to list preload files: open C:\Users\AryanSriharshaChippa\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I0204 12:09:40.652200    3860 driver.go:394] Setting default libvirt URI to qemu:///system
I0204 12:09:40.652200    3860 global.go:112] Querying for installed drivers using PATH=C:\Program Files\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Java\jdk-21\bin;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Git\cmd;C:\Program Files\apache-maven-3.9.9\bin;C:\Program Files\Java\jdk-21\bin;;C:\minikube;C:\Users\AryanSriharshaChippa\AppData\Local\Microsoft\WindowsApps;C:\Users\AryanSriharshaChippa\AppData\Local\Programs\Microsoft VS Code\bin
I0204 12:09:40.658579    3860 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0204 12:09:40.733421    3860 docker.go:123] docker version: linux-27.4.0:Docker Desktop 4.37.1 (178610)
I0204 12:09:40.736423    3860 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0204 12:09:41.829709    3860 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.0932859s)
I0204 12:09:41.830209    3860 info.go:266] docker info: {ID:1db6f9ae-3413-4352-815e-953118f2cd04 Containers:8 ContainersRunning:1 ContainersPaused:0 ContainersStopped:7 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:52 OomKillDisable:true NGoroutines:79 SystemTime:2025-02-04 06:39:41.819847162 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8157941760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}
I0204 12:09:41.830710    3860 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0204 12:09:41.835209    3860 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0204 12:09:41.835209    3860 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0204 12:09:42.961082    3860 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0204 12:09:42.966285    3860 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0204 12:09:42.974887    3860 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0204 12:09:42.974887    3860 driver.go:316] not recommending "ssh" due to default: false
I0204 12:09:42.974887    3860 driver.go:351] Picked: docker
I0204 12:09:42.975484    3860 driver.go:352] Alternatives: [hyperv ssh]
I0204 12:09:42.975484    3860 driver.go:353] Rejects: [vmware podman qemu2 virtualbox]
I0204 12:09:43.011878    3860 out.go:177] * Automatically selected the docker driver. Other choices: hyperv, ssh
I0204 12:09:43.012767    3860 start.go:297] selected driver: docker
I0204 12:09:43.012767    3860 start.go:901] validating driver "docker" against <nil>
I0204 12:09:43.012767    3860 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0204 12:09:43.018386    3860 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0204 12:09:43.196372    3860 info.go:266] docker info: {ID:1db6f9ae-3413-4352-815e-953118f2cd04 Containers:8 ContainersRunning:1 ContainersPaused:0 ContainersStopped:7 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:52 OomKillDisable:true NGoroutines:79 SystemTime:2025-02-04 06:39:43.188508425 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8157941760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}
I0204 12:09:43.196874    3860 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0204 12:09:43.219306    3860 start_flags.go:393] Using suggested 4000MB memory alloc based on sys=16051MB, container=7780MB
I0204 12:09:43.219806    3860 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0204 12:09:43.220327    3860 out.go:177] * Using Docker Desktop driver with root privileges
I0204 12:09:43.221306    3860 cni.go:84] Creating CNI manager for ""
I0204 12:09:43.221306    3860 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0204 12:09:43.221306    3860 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0204 12:09:43.221306    3860 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\AryanSriharshaChippa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0204 12:09:43.221825    3860 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0204 12:09:43.222306    3860 cache.go:121] Beginning downloading kic base image for docker with docker
I0204 12:09:43.222806    3860 out.go:177] * Pulling base image v0.0.46 ...
I0204 12:09:43.223328    3860 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0204 12:09:43.223328    3860 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0204 12:09:43.290121    3860 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0204 12:09:43.290621    3860 localpath.go:146] windows sanitize: C:\Users\AryanSriharshaChippa\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\AryanSriharshaChippa\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0204 12:09:43.290621    3860 localpath.go:146] windows sanitize: C:\Users\AryanSriharshaChippa\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\AryanSriharshaChippa\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0204 12:09:43.290621    3860 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0204 12:09:43.291122    3860 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0204 12:09:43.478344    3860 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0204 12:09:43.478344    3860 cache.go:56] Caching tarball of preloaded images
I0204 12:09:43.479183    3860 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0204 12:09:43.514728    3860 out.go:177] * Downloading Kubernetes v1.32.0 preload ...
I0204 12:09:43.516019    3860 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0204 12:09:45.812039    3860 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> C:\Users\AryanSriharshaChippa\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0204 12:10:54.839720    3860 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0204 12:10:54.896223    3860 preload.go:254] verifying checksum of C:\Users\AryanSriharshaChippa\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0204 12:10:55.496929    3860 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0204 12:10:55.497921    3860 profile.go:143] Saving config to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\config.json ...
I0204 12:10:55.497921    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\config.json: {Name:mkb5df8e9234d6f01a16a468945f1d3d4960db1e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:12:52.155918    3860 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0204 12:12:52.155918    3860 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0204 12:12:52.155918    3860 localpath.go:146] windows sanitize: C:\Users\AryanSriharshaChippa\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\AryanSriharshaChippa\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0204 12:13:14.430770    3860 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0204 12:13:14.431042    3860 cache.go:227] Successfully downloaded all kic artifacts
I0204 12:13:14.432027    3860 start.go:360] acquireMachinesLock for minikube: {Name:mk8e2f0abe51ac24dc3ceb23dff0b3aaf017e294 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0204 12:13:14.432027    3860 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0204 12:13:14.432027    3860 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\AryanSriharshaChippa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0204 12:13:14.432027    3860 start.go:125] createHost starting for "" (driver="docker")
I0204 12:13:14.433026    3860 out.go:235] * Creating docker container (CPUs=2, Memory=4000MB) ...
I0204 12:13:14.434552    3860 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0204 12:13:14.434552    3860 client.go:168] LocalClient.Create starting
I0204 12:13:14.435261    3860 main.go:141] libmachine: Creating CA: C:\Users\AryanSriharshaChippa\.minikube\certs\ca.pem
I0204 12:13:14.762849    3860 main.go:141] libmachine: Creating client certificate: C:\Users\AryanSriharshaChippa\.minikube\certs\cert.pem
I0204 12:13:14.858911    3860 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0204 12:13:14.900118    3860 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0204 12:13:14.903131    3860 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0204 12:13:14.903131    3860 cli_runner.go:164] Run: docker network inspect minikube
W0204 12:13:14.938246    3860 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0204 12:13:14.938246    3860 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0204 12:13:14.938246    3860 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0204 12:13:14.940728    3860 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0204 12:13:14.995903    3860 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001ac0d80}
I0204 12:13:14.995903    3860 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0204 12:13:14.998496    3860 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0204 12:13:15.068567    3860 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0204 12:13:15.068774    3860 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0204 12:13:15.078275    3860 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0204 12:13:15.185885    3860 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0204 12:13:15.244292    3860 oci.go:103] Successfully created a docker volume minikube
I0204 12:13:15.249423    3860 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0204 12:13:16.331685    3860 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (1.0822617s)
I0204 12:13:16.331685    3860 oci.go:107] Successfully prepared a docker volume minikube
I0204 12:13:16.331685    3860 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0204 12:13:16.331685    3860 kic.go:194] Starting extracting preloaded images to volume ...
I0204 12:13:16.337069    3860 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\AryanSriharshaChippa\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0204 12:13:21.239388    3860 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\AryanSriharshaChippa\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (4.9023188s)
I0204 12:13:21.239388    3860 kic.go:203] duration metric: took 4.907703s to extract preloaded images to volume ...
I0204 12:13:21.243834    3860 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0204 12:13:23.844356    3860 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.6005225s)
I0204 12:13:23.844356    3860 info.go:266] docker info: {ID:1db6f9ae-3413-4352-815e-953118f2cd04 Containers:8 ContainersRunning:1 ContainersPaused:0 ContainersStopped:7 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:54 OomKillDisable:true NGoroutines:83 SystemTime:2025-02-04 06:43:23.834556994 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8157941760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\AryanSriharshaChippa\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}
I0204 12:13:23.847371    3860 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0204 12:13:24.024716    3860 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0204 12:13:24.453713    3860 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0204 12:13:24.516657    3860 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0204 12:13:24.580185    3860 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0204 12:13:24.676249    3860 oci.go:144] the created container "minikube" has a running status.
I0204 12:13:24.676249    3860 kic.go:225] Creating ssh key for kic: C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa...
I0204 12:13:24.871277    3860 kic_runner.go:191] docker (temp): C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0204 12:13:24.947066    3860 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0204 12:13:25.006447    3860 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0204 12:13:25.006447    3860 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0204 12:13:25.090767    3860 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa...
I0204 12:13:25.688207    3860 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0204 12:13:25.724716    3860 machine.go:93] provisionDockerMachine start ...
I0204 12:13:25.727716    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:25.769485    3860 main.go:141] libmachine: Using SSH client type: native
I0204 12:13:25.778562    3860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6d5360] 0x6d7ea0 <nil>  [] 0s} 127.0.0.1 50878 <nil> <nil>}
I0204 12:13:25.778562    3860 main.go:141] libmachine: About to run SSH command:
hostname
I0204 12:13:25.930533    3860 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0204 12:13:25.930533    3860 ubuntu.go:169] provisioning hostname "minikube"
I0204 12:13:25.933684    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:25.972021    3860 main.go:141] libmachine: Using SSH client type: native
I0204 12:13:25.972522    3860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6d5360] 0x6d7ea0 <nil>  [] 0s} 127.0.0.1 50878 <nil> <nil>}
I0204 12:13:25.972522    3860 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0204 12:13:26.115550    3860 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0204 12:13:26.118467    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:26.155470    3860 main.go:141] libmachine: Using SSH client type: native
I0204 12:13:26.155470    3860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6d5360] 0x6d7ea0 <nil>  [] 0s} 127.0.0.1 50878 <nil> <nil>}
I0204 12:13:26.155470    3860 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0204 12:13:26.297632    3860 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0204 12:13:26.298169    3860 ubuntu.go:175] set auth options {CertDir:C:\Users\AryanSriharshaChippa\.minikube CaCertPath:C:\Users\AryanSriharshaChippa\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\AryanSriharshaChippa\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\AryanSriharshaChippa\.minikube\machines\server.pem ServerKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\AryanSriharshaChippa\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\AryanSriharshaChippa\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\AryanSriharshaChippa\.minikube}
I0204 12:13:26.298203    3860 ubuntu.go:177] setting up certificates
I0204 12:13:26.298209    3860 provision.go:84] configureAuth start
I0204 12:13:26.300861    3860 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0204 12:13:26.372984    3860 provision.go:143] copyHostCerts
I0204 12:13:26.373487    3860 exec_runner.go:151] cp: C:\Users\AryanSriharshaChippa\.minikube\certs\ca.pem --> C:\Users\AryanSriharshaChippa\.minikube/ca.pem (1115 bytes)
I0204 12:13:26.373985    3860 exec_runner.go:151] cp: C:\Users\AryanSriharshaChippa\.minikube\certs\cert.pem --> C:\Users\AryanSriharshaChippa\.minikube/cert.pem (1159 bytes)
I0204 12:13:26.374490    3860 exec_runner.go:151] cp: C:\Users\AryanSriharshaChippa\.minikube\certs\key.pem --> C:\Users\AryanSriharshaChippa\.minikube/key.pem (1675 bytes)
I0204 12:13:26.374985    3860 provision.go:117] generating server cert: C:\Users\AryanSriharshaChippa\.minikube\machines\server.pem ca-key=C:\Users\AryanSriharshaChippa\.minikube\certs\ca.pem private-key=C:\Users\AryanSriharshaChippa\.minikube\certs\ca-key.pem org=AryanSriharshaChippa.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0204 12:13:26.453860    3860 provision.go:177] copyRemoteCerts
I0204 12:13:26.458948    3860 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0204 12:13:26.461947    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:26.501696    3860 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50878 SSHKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa Username:docker}
I0204 12:13:26.595942    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1115 bytes)
I0204 12:13:26.616589    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\machines\server.pem --> /etc/docker/server.pem (1216 bytes)
I0204 12:13:26.635394    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0204 12:13:26.653562    3860 provision.go:87] duration metric: took 355.3532ms to configureAuth
I0204 12:13:26.653562    3860 ubuntu.go:193] setting minikube options for container-runtime
I0204 12:13:26.663360    3860 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0204 12:13:26.666677    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:26.702924    3860 main.go:141] libmachine: Using SSH client type: native
I0204 12:13:26.703423    3860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6d5360] 0x6d7ea0 <nil>  [] 0s} 127.0.0.1 50878 <nil> <nil>}
I0204 12:13:26.703423    3860 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0204 12:13:26.833511    3860 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0204 12:13:26.833511    3860 ubuntu.go:71] root file system type: overlay
I0204 12:13:26.833511    3860 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0204 12:13:26.836419    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:26.880495    3860 main.go:141] libmachine: Using SSH client type: native
I0204 12:13:26.880495    3860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6d5360] 0x6d7ea0 <nil>  [] 0s} 127.0.0.1 50878 <nil> <nil>}
I0204 12:13:26.880495    3860 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0204 12:13:27.029919    3860 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0204 12:13:27.033226    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:27.071636    3860 main.go:141] libmachine: Using SSH client type: native
I0204 12:13:27.071636    3860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6d5360] 0x6d7ea0 <nil>  [] 0s} 127.0.0.1 50878 <nil> <nil>}
I0204 12:13:27.071636    3860 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0204 12:13:27.870837    3860 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-02-04 06:43:27.026980246 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0204 12:13:27.870837    3860 machine.go:96] duration metric: took 2.1461212s to provisionDockerMachine
I0204 12:13:27.870837    3860 client.go:171] duration metric: took 13.4362857s to LocalClient.Create
I0204 12:13:27.870837    3860 start.go:167] duration metric: took 13.4362857s to libmachine.API.Create "minikube"
I0204 12:13:27.870837    3860 start.go:293] postStartSetup for "minikube" (driver="docker")
I0204 12:13:27.870837    3860 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0204 12:13:27.876127    3860 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0204 12:13:27.878613    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:27.916186    3860 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50878 SSHKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa Username:docker}
I0204 12:13:28.024025    3860 ssh_runner.go:195] Run: cat /etc/os-release
I0204 12:13:28.028886    3860 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0204 12:13:28.028886    3860 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0204 12:13:28.028886    3860 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0204 12:13:28.028886    3860 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0204 12:13:28.028886    3860 filesync.go:126] Scanning C:\Users\AryanSriharshaChippa\.minikube\addons for local assets ...
I0204 12:13:28.029371    3860 filesync.go:126] Scanning C:\Users\AryanSriharshaChippa\.minikube\files for local assets ...
I0204 12:13:28.029371    3860 start.go:296] duration metric: took 158.5331ms for postStartSetup
I0204 12:13:28.034370    3860 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0204 12:13:28.073992    3860 profile.go:143] Saving config to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\config.json ...
I0204 12:13:28.115942    3860 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0204 12:13:28.121551    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:28.169035    3860 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50878 SSHKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa Username:docker}
I0204 12:13:28.269167    3860 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0204 12:13:28.273652    3860 start.go:128] duration metric: took 13.8416254s to createHost
I0204 12:13:28.273652    3860 start.go:83] releasing machines lock for "minikube", held for 13.8416254s
I0204 12:13:28.276650    3860 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0204 12:13:28.317949    3860 ssh_runner.go:195] Run: cat /version.json
I0204 12:13:28.320950    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:28.338497    3860 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0204 12:13:28.342002    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:28.377034    3860 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50878 SSHKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa Username:docker}
I0204 12:13:28.383532    3860 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50878 SSHKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa Username:docker}
W0204 12:13:28.470886    3860 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0204 12:13:28.470886    3860 ssh_runner.go:195] Run: systemctl --version
I0204 12:13:28.479386    3860 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0204 12:13:28.488109    3860 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0204 12:13:28.495193    3860 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0204 12:13:28.500514    3860 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0204 12:13:28.533502    3860 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0204 12:13:28.533502    3860 start.go:495] detecting cgroup driver to use...
I0204 12:13:28.533502    3860 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0204 12:13:28.533502    3860 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0204 12:13:28.554166    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0204 12:13:28.568440    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0204 12:13:28.575979    3860 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0204 12:13:28.580954    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0204 12:13:28.593221    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0204 12:13:28.606706    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0204 12:13:28.619824    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0204 12:13:28.634311    3860 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0204 12:13:28.647320    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0204 12:13:28.661820    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0204 12:13:28.674552    3860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0204 12:13:28.687566    3860 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0204 12:13:28.699078    3860 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0204 12:13:28.710450    3860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0204 12:13:28.800440    3860 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0204 12:13:28.918337    3860 start.go:495] detecting cgroup driver to use...
I0204 12:13:28.918830    3860 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0204 12:13:28.924339    3860 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0204 12:13:28.936150    3860 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0204 12:13:28.942010    3860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0204 12:13:28.952142    3860 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0204 12:13:28.970231    3860 ssh_runner.go:195] Run: which cri-dockerd
I0204 12:13:28.978777    3860 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0204 12:13:28.985451    3860 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0204 12:13:29.004146    3860 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0204 12:13:29.095624    3860 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0204 12:13:29.191441    3860 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0204 12:13:29.191441    3860 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0204 12:13:29.212925    3860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0204 12:13:29.315401    3860 ssh_runner.go:195] Run: sudo systemctl restart docker
I0204 12:13:29.587859    3860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0204 12:13:29.602213    3860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0204 12:13:29.616833    3860 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0204 12:13:29.721750    3860 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0204 12:13:29.817590    3860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0204 12:13:29.906318    3860 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0204 12:13:29.922963    3860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0204 12:13:29.939078    3860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0204 12:13:29.982773    3860 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0204 12:13:29.984821    3860 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0204 12:13:30.039687    3860 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0204 12:13:30.091429    3860 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0204 12:13:30.097525    3860 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0204 12:13:30.100221    3860 start.go:563] Will wait 60s for crictl version
I0204 12:13:30.105952    3860 ssh_runner.go:195] Run: which crictl
I0204 12:13:30.113732    3860 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0204 12:13:30.141693    3860 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0204 12:13:30.144638    3860 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0204 12:13:30.166620    3860 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0204 12:13:30.222607    3860 out.go:235] * Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0204 12:13:30.228457    3860 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0204 12:13:30.298823    3860 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0204 12:13:30.303338    3860 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0204 12:13:30.306823    3860 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0204 12:13:30.317529    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0204 12:13:30.352926    3860 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\AryanSriharshaChippa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0204 12:13:30.352926    3860 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0204 12:13:30.355926    3860 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0204 12:13:30.372618    3860 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0204 12:13:30.372618    3860 docker.go:619] Images already preloaded, skipping extraction
I0204 12:13:30.375310    3860 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0204 12:13:30.391617    3860 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0204 12:13:30.391617    3860 cache_images.go:84] Images are preloaded, skipping loading
I0204 12:13:30.391617    3860 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0204 12:13:30.391617    3860 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0204 12:13:30.394389    3860 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0204 12:13:30.428540    3860 cni.go:84] Creating CNI manager for ""
I0204 12:13:30.428540    3860 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0204 12:13:30.428540    3860 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0204 12:13:30.428540    3860 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0204 12:13:30.428540    3860 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0204 12:13:30.433410    3860 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0204 12:13:30.440032    3860 binaries.go:44] Found k8s binaries, skipping transfer
I0204 12:13:30.445058    3860 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0204 12:13:30.451683    3860 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0204 12:13:30.464941    3860 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0204 12:13:30.478437    3860 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0204 12:13:30.496710    3860 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0204 12:13:30.500246    3860 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0204 12:13:30.513421    3860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0204 12:13:30.609572    3860 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0204 12:13:30.622904    3860 certs.go:68] Setting up C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube for IP: 192.168.49.2
I0204 12:13:30.622904    3860 certs.go:194] generating shared ca certs ...
I0204 12:13:30.622904    3860 certs.go:226] acquiring lock for ca certs: {Name:mk321581ad4c6d446fc5c07f9e1d9a433227e0b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:30.622904    3860 certs.go:240] generating "minikubeCA" ca cert: C:\Users\AryanSriharshaChippa\.minikube\ca.key
I0204 12:13:30.746927    3860 crypto.go:156] Writing cert to C:\Users\AryanSriharshaChippa\.minikube\ca.crt ...
I0204 12:13:30.746927    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\ca.crt: {Name:mk0934e860ebed44e2f953bb80571a6c40cfea99 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:30.746927    3860 crypto.go:164] Writing key to C:\Users\AryanSriharshaChippa\.minikube\ca.key ...
I0204 12:13:30.746927    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\ca.key: {Name:mkb7ae6aef29d60eae15dc83a153ac932f6d5558 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:30.747938    3860 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\AryanSriharshaChippa\.minikube\proxy-client-ca.key
I0204 12:13:30.857004    3860 crypto.go:156] Writing cert to C:\Users\AryanSriharshaChippa\.minikube\proxy-client-ca.crt ...
I0204 12:13:30.857004    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\proxy-client-ca.crt: {Name:mk20e075c5fd9b99c444c470c291a74a4c654068 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:30.857935    3860 crypto.go:164] Writing key to C:\Users\AryanSriharshaChippa\.minikube\proxy-client-ca.key ...
I0204 12:13:30.857935    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\proxy-client-ca.key: {Name:mk151a614a8da8119ac3a5992f7e87b877b6e38f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:30.858953    3860 certs.go:256] generating profile certs ...
I0204 12:13:30.858953    3860 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\client.key
I0204 12:13:30.858953    3860 crypto.go:68] Generating cert C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\client.crt with IP's: []
I0204 12:13:30.916212    3860 crypto.go:156] Writing cert to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\client.crt ...
I0204 12:13:30.916212    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\client.crt: {Name:mke033bc4484dabeb20538c3a1fe207498d64c0a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:30.916212    3860 crypto.go:164] Writing key to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\client.key ...
I0204 12:13:30.916212    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\client.key: {Name:mk53cc24f9892981b87b6b19b16341b6e5ceef3d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:30.917643    3860 certs.go:363] generating signed profile cert for "minikube": C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0204 12:13:30.917643    3860 crypto.go:68] Generating cert C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0204 12:13:31.053240    3860 crypto.go:156] Writing cert to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0204 12:13:31.053240    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk5015a96817be9d1d3376b0fb5698272ac16247 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:31.054193    3860 crypto.go:164] Writing key to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0204 12:13:31.054193    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk93006457d3b917c770f695e3ab76a77c5fa238 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:31.055153    3860 certs.go:381] copying C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.crt
I0204 12:13:31.143417    3860 certs.go:385] copying C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.key
I0204 12:13:31.144505    3860 certs.go:363] generating signed profile cert for "aggregator": C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.key
I0204 12:13:31.145039    3860 crypto.go:68] Generating cert C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0204 12:13:31.332549    3860 crypto.go:156] Writing cert to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.crt ...
I0204 12:13:31.332549    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.crt: {Name:mke22f2430abd650535c8471ab6635940344e2a8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:31.333451    3860 crypto.go:164] Writing key to C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.key ...
I0204 12:13:31.333451    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.key: {Name:mkc252ede14dbe0f6866e6ac37e08d058e52d404 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:31.346457    3860 certs.go:484] found cert: C:\Users\AryanSriharshaChippa\.minikube\certs\ca-key.pem (1675 bytes)
I0204 12:13:31.347527    3860 certs.go:484] found cert: C:\Users\AryanSriharshaChippa\.minikube\certs\ca.pem (1115 bytes)
I0204 12:13:31.347527    3860 certs.go:484] found cert: C:\Users\AryanSriharshaChippa\.minikube\certs\cert.pem (1159 bytes)
I0204 12:13:31.347527    3860 certs.go:484] found cert: C:\Users\AryanSriharshaChippa\.minikube\certs\key.pem (1675 bytes)
I0204 12:13:31.348538    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0204 12:13:31.368022    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0204 12:13:31.385704    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0204 12:13:31.403265    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0204 12:13:31.421147    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0204 12:13:31.439147    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0204 12:13:31.456849    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0204 12:13:31.474153    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0204 12:13:31.491514    3860 ssh_runner.go:362] scp C:\Users\AryanSriharshaChippa\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0204 12:13:31.512415    3860 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0204 12:13:31.531831    3860 ssh_runner.go:195] Run: openssl version
I0204 12:13:31.541347    3860 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0204 12:13:31.553377    3860 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0204 12:13:31.557216    3860 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb  4 06:43 /usr/share/ca-certificates/minikubeCA.pem
I0204 12:13:31.562159    3860 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0204 12:13:31.573059    3860 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0204 12:13:31.585410    3860 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0204 12:13:31.589821    3860 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0204 12:13:31.589821    3860 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\AryanSriharshaChippa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0204 12:13:31.592611    3860 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0204 12:13:31.611819    3860 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0204 12:13:31.624158    3860 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0204 12:13:31.632649    3860 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0204 12:13:31.637607    3860 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0204 12:13:31.644574    3860 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0204 12:13:31.644574    3860 kubeadm.go:157] found existing configuration files:

I0204 12:13:31.649645    3860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0204 12:13:31.657150    3860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0204 12:13:31.662651    3860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0204 12:13:31.675003    3860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0204 12:13:31.683498    3860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0204 12:13:31.689010    3860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0204 12:13:31.702593    3860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0204 12:13:31.709593    3860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0204 12:13:31.715092    3860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0204 12:13:31.728427    3860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0204 12:13:31.737001    3860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0204 12:13:31.741976    3860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0204 12:13:31.749856    3860 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0204 12:13:31.794253    3860 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0204 12:13:31.798048    3860 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0204 12:13:31.853625    3860 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0204 12:13:40.093899    3860 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0204 12:13:40.093899    3860 kubeadm.go:310] [preflight] Running pre-flight checks
I0204 12:13:40.094414    3860 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0204 12:13:40.094414    3860 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0204 12:13:40.094414    3860 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0204 12:13:40.094414    3860 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0204 12:13:40.094899    3860 out.go:235]   - Generating certificates and keys ...
I0204 12:13:40.095899    3860 kubeadm.go:310] [certs] Using existing ca certificate authority
I0204 12:13:40.095899    3860 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0204 12:13:40.095899    3860 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0204 12:13:40.095899    3860 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0204 12:13:40.095899    3860 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0204 12:13:40.095899    3860 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0204 12:13:40.096399    3860 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0204 12:13:40.096399    3860 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0204 12:13:40.096399    3860 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0204 12:13:40.096399    3860 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0204 12:13:40.096899    3860 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0204 12:13:40.096899    3860 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0204 12:13:40.096899    3860 kubeadm.go:310] [certs] Generating "sa" key and public key
I0204 12:13:40.096899    3860 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0204 12:13:40.096899    3860 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0204 12:13:40.096899    3860 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0204 12:13:40.097325    3860 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0204 12:13:40.097401    3860 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0204 12:13:40.097401    3860 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0204 12:13:40.097401    3860 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0204 12:13:40.097401    3860 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0204 12:13:40.097900    3860 out.go:235]   - Booting up control plane ...
I0204 12:13:40.098398    3860 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0204 12:13:40.098398    3860 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0204 12:13:40.098398    3860 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0204 12:13:40.098398    3860 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0204 12:13:40.098398    3860 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0204 12:13:40.098398    3860 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0204 12:13:40.098900    3860 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0204 12:13:40.098900    3860 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0204 12:13:40.098900    3860 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.951935ms
I0204 12:13:40.098900    3860 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0204 12:13:40.098900    3860 kubeadm.go:310] [api-check] The API server is healthy after 4.501767619s
I0204 12:13:40.099399    3860 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0204 12:13:40.099399    3860 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0204 12:13:40.099399    3860 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0204 12:13:40.099899    3860 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0204 12:13:40.099899    3860 kubeadm.go:310] [bootstrap-token] Using token: sj7rg6.f9s8i7o2cmkvs88h
I0204 12:13:40.100398    3860 out.go:235]   - Configuring RBAC rules ...
I0204 12:13:40.100899    3860 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0204 12:13:40.100899    3860 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0204 12:13:40.101400    3860 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0204 12:13:40.101400    3860 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0204 12:13:40.101899    3860 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0204 12:13:40.101899    3860 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0204 12:13:40.101899    3860 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0204 12:13:40.101899    3860 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0204 12:13:40.102414    3860 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0204 12:13:40.102414    3860 kubeadm.go:310] 
I0204 12:13:40.102414    3860 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0204 12:13:40.102414    3860 kubeadm.go:310] 
I0204 12:13:40.102414    3860 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0204 12:13:40.102414    3860 kubeadm.go:310] 
I0204 12:13:40.102414    3860 kubeadm.go:310]   mkdir -p $HOME/.kube
I0204 12:13:40.102414    3860 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0204 12:13:40.102414    3860 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0204 12:13:40.102414    3860 kubeadm.go:310] 
I0204 12:13:40.102414    3860 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0204 12:13:40.102414    3860 kubeadm.go:310] 
I0204 12:13:40.102915    3860 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0204 12:13:40.102915    3860 kubeadm.go:310] 
I0204 12:13:40.102915    3860 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0204 12:13:40.102915    3860 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0204 12:13:40.102915    3860 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0204 12:13:40.102915    3860 kubeadm.go:310] 
I0204 12:13:40.102915    3860 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0204 12:13:40.103415    3860 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0204 12:13:40.103415    3860 kubeadm.go:310] 
I0204 12:13:40.103415    3860 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token sj7rg6.f9s8i7o2cmkvs88h \
I0204 12:13:40.103415    3860 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:3300b303c00ed1b7f89cf4a805221733ba6e907ff6506b8707fed7d468a15a66 \
I0204 12:13:40.103415    3860 kubeadm.go:310] 	--control-plane 
I0204 12:13:40.103415    3860 kubeadm.go:310] 
I0204 12:13:40.103415    3860 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0204 12:13:40.103415    3860 kubeadm.go:310] 
I0204 12:13:40.103899    3860 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token sj7rg6.f9s8i7o2cmkvs88h \
I0204 12:13:40.103899    3860 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:3300b303c00ed1b7f89cf4a805221733ba6e907ff6506b8707fed7d468a15a66 
I0204 12:13:40.103899    3860 cni.go:84] Creating CNI manager for ""
I0204 12:13:40.103899    3860 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0204 12:13:40.104403    3860 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0204 12:13:40.109898    3860 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0204 12:13:40.116910    3860 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0204 12:13:40.131105    3860 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0204 12:13:40.138104    3860 ops.go:34] apiserver oom_adj: -16
I0204 12:13:40.138104    3860 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0204 12:13:40.139106    3860 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_02_04T12_13_40_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0204 12:13:40.194372    3860 kubeadm.go:1113] duration metric: took 63.2676ms to wait for elevateKubeSystemPrivileges
I0204 12:13:40.211412    3860 kubeadm.go:394] duration metric: took 8.621591s to StartCluster
I0204 12:13:40.211412    3860 settings.go:142] acquiring lock: {Name:mk387c48ae462ceeb02085201e7867e3e7b0cc1b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:40.211412    3860 settings.go:150] Updating kubeconfig:  C:\Users\AryanSriharshaChippa\.kube\config
I0204 12:13:40.211928    3860 lock.go:35] WriteFile acquiring C:\Users\AryanSriharshaChippa\.kube\config: {Name:mk87eaa6c29cdb696c7f887a15a47f3af5efed84 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0204 12:13:40.212939    3860 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0204 12:13:40.212980    3860 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0204 12:13:40.212980    3860 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0204 12:13:40.212980    3860 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0204 12:13:40.212980    3860 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0204 12:13:40.212980    3860 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0204 12:13:40.212980    3860 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0204 12:13:40.213450    3860 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0204 12:13:40.213524    3860 host.go:66] Checking if "minikube" exists ...
I0204 12:13:40.213684    3860 out.go:177] * Verifying Kubernetes components...
I0204 12:13:40.221945    3860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0204 12:13:40.225449    3860 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0204 12:13:40.225948    3860 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0204 12:13:40.273706    3860 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0204 12:13:40.274208    3860 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0204 12:13:40.274208    3860 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0204 12:13:40.274706    3860 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0204 12:13:40.274706    3860 host.go:66] Checking if "minikube" exists ...
I0204 12:13:40.278205    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:40.285199    3860 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0204 12:13:40.309493    3860 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0204 12:13:40.333202    3860 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50878 SSHKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa Username:docker}
I0204 12:13:40.338716    3860 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0204 12:13:40.338716    3860 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0204 12:13:40.341730    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0204 12:13:40.381366    3860 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50878 SSHKeyPath:C:\Users\AryanSriharshaChippa\.minikube\machines\minikube\id_rsa Username:docker}
I0204 12:13:40.396667    3860 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0204 12:13:40.492825    3860 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0204 12:13:40.493821    3860 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0204 12:13:40.603740    3860 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0204 12:13:40.607762    3860 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0204 12:13:40.644842    3860 api_server.go:52] waiting for apiserver process to appear ...
I0204 12:13:40.649689    3860 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0204 12:13:41.005233    3860 api_server.go:72] duration metric: took 792.2523ms to wait for apiserver process to appear ...
I0204 12:13:41.005233    3860 api_server.go:88] waiting for apiserver healthz status ...
I0204 12:13:41.005233    3860 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50877/healthz ...
I0204 12:13:41.010135    3860 api_server.go:279] https://127.0.0.1:50877/healthz returned 200:
ok
I0204 12:13:41.010810    3860 api_server.go:141] control plane version: v1.32.0
I0204 12:13:41.010810    3860 api_server.go:131] duration metric: took 5.5774ms to wait for apiserver health ...
I0204 12:13:41.010810    3860 system_pods.go:43] waiting for kube-system pods to appear ...
I0204 12:13:41.013332    3860 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0204 12:13:41.013861    3860 addons.go:514] duration metric: took 800.8805ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0204 12:13:41.015351    3860 system_pods.go:59] 5 kube-system pods found
I0204 12:13:41.015351    3860 system_pods.go:61] "etcd-minikube" [b7044417-86ff-40cf-a14c-508c9b3a682c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0204 12:13:41.015351    3860 system_pods.go:61] "kube-apiserver-minikube" [5c37f821-1a0c-40c5-a52e-53954ebd7e16] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0204 12:13:41.015351    3860 system_pods.go:61] "kube-controller-manager-minikube" [816a4368-0e93-4c68-a73b-9f172e6ebe1b] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0204 12:13:41.015351    3860 system_pods.go:61] "kube-scheduler-minikube" [27737a4d-360f-417f-836d-b79e83de7a6e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0204 12:13:41.015351    3860 system_pods.go:61] "storage-provisioner" [2db0edc3-3f42-4674-8314-67c299349d88] Pending
I0204 12:13:41.015351    3860 system_pods.go:74] duration metric: took 4.5404ms to wait for pod list to return data ...
I0204 12:13:41.015351    3860 kubeadm.go:582] duration metric: took 802.3701ms to wait for: map[apiserver:true system_pods:true]
I0204 12:13:41.015351    3860 node_conditions.go:102] verifying NodePressure condition ...
I0204 12:13:41.018509    3860 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0204 12:13:41.018509    3860 node_conditions.go:123] node cpu capacity is 8
I0204 12:13:41.018509    3860 node_conditions.go:105] duration metric: took 3.1587ms to run NodePressure ...
I0204 12:13:41.018509    3860 start.go:241] waiting for startup goroutines ...
I0204 12:13:41.109011    3860 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0204 12:13:41.109011    3860 start.go:246] waiting for cluster config update ...
I0204 12:13:41.109011    3860 start.go:255] writing updated cluster config ...
I0204 12:13:41.114409    3860 ssh_runner.go:195] Run: rm -f paused
I0204 12:13:41.266307    3860 start.go:600] kubectl: 1.30.5, cluster: 1.32.0 (minor skew: 2)
I0204 12:13:41.267345    3860 out.go:201] 
W0204 12:13:41.268217    3860 out.go:270] ! C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.30.5, which may have incompatibilities with Kubernetes 1.32.0.
I0204 12:13:41.269418    3860 out.go:177]   - Want kubectl v1.32.0? Try 'minikube kubectl -- get pods -A'
I0204 12:13:41.270180    3860 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.195072010Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.195078177Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.195082414Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.195100097Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.195134204Z" level=info msg="Daemon has completed initialization"
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.237171984Z" level=info msg="API listen on /var/run/docker.sock"
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.237196408Z" level=info msg="API listen on [::]:2376"
Feb 04 06:43:29 minikube systemd[1]: Started Docker Application Container Engine.
Feb 04 06:43:29 minikube systemd[1]: Stopping Docker Application Container Engine...
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.324736284Z" level=info msg="Processing signal 'terminated'"
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.325959651Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.326584330Z" level=info msg="Daemon shutdown complete"
Feb 04 06:43:29 minikube dockerd[1106]: time="2025-02-04T06:43:29.326663580Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Feb 04 06:43:29 minikube systemd[1]: docker.service: Deactivated successfully.
Feb 04 06:43:29 minikube systemd[1]: Stopped Docker Application Container Engine.
Feb 04 06:43:29 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.359014957Z" level=info msg="Starting up"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.360117337Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.374449769Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.396325031Z" level=info msg="Loading containers: start."
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.486384844Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.531232343Z" level=info msg="Loading containers: done."
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.548848511Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.548890978Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.548896829Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.548900487Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.548916688Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.548946170Z" level=info msg="Daemon has completed initialization"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.580356523Z" level=info msg="API listen on [::]:2376"
Feb 04 06:43:29 minikube dockerd[1381]: time="2025-02-04T06:43:29.580376476Z" level=info msg="API listen on /var/run/docker.sock"
Feb 04 06:43:29 minikube systemd[1]: Started Docker Application Container Engine.
Feb 04 06:43:30 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Start docker client with request timeout 0s"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Loaded network plugin cni"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 04 06:43:30 minikube cri-dockerd[1660]: time="2025-02-04T06:43:30Z" level=info msg="Start cri-dockerd grpc backend"
Feb 04 06:43:30 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 04 06:43:34 minikube cri-dockerd[1660]: time="2025-02-04T06:43:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e979cf3daddcec5adcdffb457838fc53f9c16da3473a458a5b00edbc0c285164/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 04 06:43:34 minikube cri-dockerd[1660]: time="2025-02-04T06:43:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1f7e1a8f57af0be2890bdc21c43b6296b20d6f5b4501c2caaf6fb96e04e3ba8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 04 06:43:34 minikube cri-dockerd[1660]: time="2025-02-04T06:43:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/292d25245a514e3784312eacaf245267b9f7dbaeda8d5f5cbb9829f2bcdf948c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 04 06:43:34 minikube cri-dockerd[1660]: time="2025-02-04T06:43:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db7051ad58ee17cf3bea9741e85f2e293f156ec2c77ab566f4e7ba2faa7cd319/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 04 06:43:45 minikube cri-dockerd[1660]: time="2025-02-04T06:43:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0805f564b2c52124078d3ee06f47bdbf3692e8751dff3e7259ab2afb4438d81a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 04 06:43:45 minikube cri-dockerd[1660]: time="2025-02-04T06:43:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c1cfa5cf4b32ec186fe163d17ddda7f4d842e617482fe4796e30e8398c146aed/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 04 06:43:45 minikube cri-dockerd[1660]: time="2025-02-04T06:43:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/48e5cc7b1e03b7af5359b8afc34747c58803585234f6b55337d44a98370d66db/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 04 06:43:49 minikube cri-dockerd[1660]: time="2025-02-04T06:43:49Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 04 06:47:33 minikube dockerd[1381]: time="2025-02-04T06:47:33.332360741Z" level=error msg=/moby.buildkit.v1.frontend.LLBBridge/Solve error="rpc error: code = Unknown desc = failed to read dockerfile: open Dockerfile: no such file or directory" spanID=2d5a5215ad4968eb traceID=e579176cf9ef82f8639acfd7575810c0
Feb 04 06:47:33 minikube dockerd[1381]: time="2025-02-04T06:47:33.378439961Z" level=error msg=/moby.buildkit.v1.Control/Solve error="rpc error: code = Unknown desc = failed to read dockerfile: open Dockerfile: no such file or directory" spanID=4d52ccf77c019df3 traceID=e579176cf9ef82f8639acfd7575810c0
Feb 04 06:55:28 minikube cri-dockerd[1660]: time="2025-02-04T06:55:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/83fde1fb97b3a34eed3f4ef08c49614cfcb13bf4b7159362e597a8639b5bce6f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 04 06:55:32 minikube dockerd[1381]: time="2025-02-04T06:55:32.208158877Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 04 06:55:32 minikube dockerd[1381]: time="2025-02-04T06:55:32.208203251Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 04 06:55:50 minikube dockerd[1381]: time="2025-02-04T06:55:50.733495603Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 04 06:55:50 minikube dockerd[1381]: time="2025-02-04T06:55:50.733542302Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 04 06:56:24 minikube dockerd[1381]: time="2025-02-04T06:56:24.254866441Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 04 06:56:24 minikube dockerd[1381]: time="2025-02-04T06:56:24.254933447Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
a3266c33711e6       6e38f40d628db       13 minutes ago      Running             storage-provisioner       0                   48e5cc7b1e03b       storage-provisioner
902e1161369f0       c69fa2e9cbf5f       13 minutes ago      Running             coredns                   0                   c1cfa5cf4b32e       coredns-668d6bf9bc-s2745
99ca2c9f91854       040f9f8aac8cd       13 minutes ago      Running             kube-proxy                0                   0805f564b2c52       kube-proxy-99sk6
002d93aa3ddf4       a389e107f4ff1       13 minutes ago      Running             kube-scheduler            0                   db7051ad58ee1       kube-scheduler-minikube
83b196e696117       8cab3d2a8bd0f       13 minutes ago      Running             kube-controller-manager   0                   292d25245a514       kube-controller-manager-minikube
8776ebe75d653       c2e17b8d0f4a3       13 minutes ago      Running             kube-apiserver            0                   a1f7e1a8f57af       kube-apiserver-minikube
fe6b74cba3f28       a9e7e6b294baf       13 minutes ago      Running             etcd                      0                   e979cf3daddce       etcd-minikube


==> coredns [902e1161369f] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:43305 - 41309 "HINFO IN 6934199620156563612.2940998316901129498. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.089382913s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1342861031]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (04-Feb-2025 06:43:45.499) (total time: 21043ms):
Trace[1342861031]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21043ms (06:44:06.541)
Trace[1342861031]: [21.043914287s] [21.043914287s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1720219219]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (04-Feb-2025 06:43:45.499) (total time: 21044ms):
Trace[1720219219]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21043ms (06:44:06.541)
Trace[1720219219]: [21.044057506s] [21.044057506s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[569349783]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (04-Feb-2025 06:43:45.499) (total time: 21044ms):
Trace[569349783]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21043ms (06:44:06.541)
Trace[569349783]: [21.044128849s] [21.044128849s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_04T12_13_40_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 04 Feb 2025 06:43:37 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 04 Feb 2025 06:56:58 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 04 Feb 2025 06:51:48 +0000   Tue, 04 Feb 2025 06:43:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 04 Feb 2025 06:51:48 +0000   Tue, 04 Feb 2025 06:43:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 04 Feb 2025 06:51:48 +0000   Tue, 04 Feb 2025 06:43:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 04 Feb 2025 06:51:48 +0000   Tue, 04 Feb 2025 06:43:37 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7966740Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7966740Ki
  pods:               110
System Info:
  Machine ID:                 ece7a0df1095413ca1428ad1bd0bc1c3
  System UUID:                ece7a0df1095413ca1428ad1bd0bc1c3
  Boot ID:                    7216d3d0-7b84-4091-aac8-6822ae636342
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                     ------------  ----------  ---------------  -------------  ---
  default                     static-web-deployment-c66958b95-bxkmg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         93s
  kube-system                 coredns-668d6bf9bc-s2745                 100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     13m
  kube-system                 etcd-minikube                            100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         13m
  kube-system                 kube-apiserver-minikube                  250m (3%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-controller-manager-minikube         200m (2%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-proxy-99sk6                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-scheduler-minikube                  100m (1%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 storage-provisioner                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age   From             Message
  ----     ------                             ----  ----             -------
  Normal   Starting                           13m   kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  13m   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           13m   kubelet          Starting kubelet.
  Warning  CgroupV1                           13m   kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            13m   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            13m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              13m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               13m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     13m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Feb 4 06:37] PCI: Fatal: No config space access function found
[  +0.021495] PCI: System does not support PCI
[  +0.024098] kvm: no hardware support
[  +0.000005] kvm: no hardware support
[  +3.031429] FS-Cache: Duplicate cookie detected
[  +0.019444] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.000955] FS-Cache: O-cookie d=000000004cd56997{9P.session} n=0000000037df5399
[  +0.023604] FS-Cache: O-key=[10] '34323934393337363036'
[  +0.004631] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.002341] FS-Cache: N-cookie d=000000004cd56997{9P.session} n=000000009f582446
[  +0.004096] FS-Cache: N-key=[10] '34323934393337363036'
[  +1.170200] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.011783] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.630582] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.022712] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000715] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003342] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001413] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.842014] netlink: 'init': attribute type 4 has an invalid length.
[Feb 4 06:43] tmpfs: Unknown parameter 'noswap'
[  +5.566528] tmpfs: Unknown parameter 'noswap'


==> etcd [fe6b74cba3f2] <==
{"level":"warn","ts":"2025-02-04T06:43:34.890216Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-04T06:43:34.890297Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-02-04T06:43:34.890350Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-04T06:43:34.890356Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-04T06:43:34.890376Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-04T06:43:34.890710Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-02-04T06:43:34.890768Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-02-04T06:43:34.893765Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.690672ms"}
{"level":"info","ts":"2025-02-04T06:43:34.898428Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-02-04T06:43:34.898512Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-02-04T06:43:34.898553Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-02-04T06:43:34.898562Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-02-04T06:43:34.898585Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-02-04T06:43:34.898627Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-02-04T06:43:34.903070Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-04T06:43:34.904266Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-02-04T06:43:34.905270Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-04T06:43:34.906619Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-02-04T06:43:34.906720Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-02-04T06:43:34.906843Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-04T06:43:34.906890Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-04T06:43:34.906903Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-04T06:43:34.906949Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-04T06:43:34.907584Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-02-04T06:43:34.907929Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-04T06:43:34.910596Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-04T06:43:34.910680Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-04T06:43:34.910710Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-04T06:43:34.910851Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-04T06:43:34.910877Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-04T06:43:35.499753Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-02-04T06:43:35.499816Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-02-04T06:43:35.499852Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-02-04T06:43:35.499874Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-02-04T06:43:35.499880Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-04T06:43:35.499889Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-02-04T06:43:35.499899Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-04T06:43:35.535972Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-04T06:43:35.567776Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-04T06:43:35.568277Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-04T06:43:35.568315Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-04T06:43:35.567885Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-04T06:43:35.567982Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-04T06:43:35.569501Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-04T06:43:35.569642Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-04T06:43:35.571081Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-04T06:43:35.571143Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-04T06:43:35.575494Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-04T06:43:35.575628Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-04T06:43:35.575676Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-04T06:43:37.175219Z","caller":"traceutil/trace.go:171","msg":"trace[1216228043] linearizableReadLoop","detail":"{readStateIndex:5; appliedIndex:4; }","duration":"105.143262ms","start":"2025-02-04T06:43:37.070046Z","end":"2025-02-04T06:43:37.175190Z","steps":["trace[1216228043] 'read index received'  (duration: 105.004169ms)","trace[1216228043] 'applied index is now lower than readState.Index'  (duration: 137.995µs)"],"step_count":2}
{"level":"warn","ts":"2025-02-04T06:43:37.175469Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.387654ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-02-04T06:43:37.175540Z","caller":"traceutil/trace.go:171","msg":"trace[1051422300] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:0; response_revision:2; }","duration":"105.508346ms","start":"2025-02-04T06:43:37.070013Z","end":"2025-02-04T06:43:37.175521Z","steps":["trace[1051422300] 'agreement among raft nodes before linearized reading'  (duration: 105.333221ms)"],"step_count":1}
{"level":"info","ts":"2025-02-04T06:43:37.175538Z","caller":"traceutil/trace.go:171","msg":"trace[2092888329] transaction","detail":"{read_only:false; response_revision:2; number_of_response:1; }","duration":"106.982907ms","start":"2025-02-04T06:43:37.068505Z","end":"2025-02-04T06:43:37.175488Z","steps":["trace[2092888329] 'process raft request'  (duration: 106.424791ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-04T06:43:37.181304Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.773055ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:2874"}
{"level":"info","ts":"2025-02-04T06:43:37.181679Z","caller":"traceutil/trace.go:171","msg":"trace[1617921544] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:9; }","duration":"106.189545ms","start":"2025-02-04T06:43:37.075466Z","end":"2025-02-04T06:43:37.181656Z","steps":["trace[1617921544] 'agreement among raft nodes before linearized reading'  (duration: 105.742519ms)"],"step_count":1}
{"level":"info","ts":"2025-02-04T06:53:35.976857Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":608}
{"level":"info","ts":"2025-02-04T06:53:35.981593Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":608,"took":"4.517079ms","hash":522623110,"current-db-size-bytes":1347584,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":1347584,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-02-04T06:53:35.981638Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":522623110,"revision":608,"compact-revision":-1}


==> kernel <==
 06:57:01 up 19 min,  0 users,  load average: 0.02, 0.12, 0.09
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [8776ebe75d65] <==
I0204 06:43:36.987280       1 local_available_controller.go:156] Starting LocalAvailability controller
I0204 06:43:36.987289       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0204 06:43:36.987367       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0204 06:43:36.987393       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0204 06:43:36.987402       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0204 06:43:36.987411       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0204 06:43:36.998886       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0204 06:43:36.998953       1 controller.go:90] Starting OpenAPI V3 controller
I0204 06:43:36.998964       1 naming_controller.go:294] Starting NamingConditionController
I0204 06:43:36.998980       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0204 06:43:36.998991       1 crd_finalizer.go:269] Starting CRDFinalizer
I0204 06:43:36.998985       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0204 06:43:36.999021       1 aggregator.go:169] waiting for initial CRD sync...
I0204 06:43:36.999027       1 establishing_controller.go:81] Starting EstablishingController
I0204 06:43:36.999251       1 controller.go:142] Starting OpenAPI controller
I0204 06:43:36.999397       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0204 06:43:36.999423       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0204 06:43:37.017884       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0204 06:43:37.017960       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0204 06:43:37.017995       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0204 06:43:37.018016       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0204 06:43:37.018060       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0204 06:43:37.018194       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0204 06:43:37.018332       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0204 06:43:37.018427       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0204 06:43:37.078487       1 shared_informer.go:320] Caches are synced for node_authorizer
I0204 06:43:37.087024       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0204 06:43:37.087078       1 policy_source.go:240] refreshing policies
I0204 06:43:37.087143       1 shared_informer.go:320] Caches are synced for configmaps
I0204 06:43:37.087550       1 cache.go:39] Caches are synced for LocalAvailability controller
I0204 06:43:37.087845       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0204 06:43:37.088658       1 controller.go:615] quota admission added evaluator for: namespaces
I0204 06:43:37.167955       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0204 06:43:37.168127       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0204 06:43:37.168420       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0204 06:43:37.168484       1 aggregator.go:171] initial CRD sync complete...
I0204 06:43:37.168498       1 autoregister_controller.go:144] Starting autoregister controller
I0204 06:43:37.168509       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0204 06:43:37.168520       1 cache.go:39] Caches are synced for autoregister controller
I0204 06:43:37.168686       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0204 06:43:37.168856       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0204 06:43:37.168880       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
E0204 06:43:37.185286       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0204 06:43:37.268410       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0204 06:43:38.041139       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0204 06:43:38.044967       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0204 06:43:38.044992       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0204 06:43:38.519402       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0204 06:43:38.556013       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0204 06:43:38.692995       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0204 06:43:38.701658       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0204 06:43:38.703071       1 controller.go:615] quota admission added evaluator for: endpoints
I0204 06:43:38.708084       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0204 06:43:39.079789       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0204 06:43:39.499149       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0204 06:43:39.511800       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0204 06:43:39.521944       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0204 06:43:43.681550       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0204 06:43:44.481598       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0204 06:55:43.507462       1 alloc.go:330] "allocated clusterIPs" service="default/static-web-service" clusterIPs={"IPv4":"10.97.236.46"}


==> kube-controller-manager [83b196e69611] <==
I0204 06:43:43.578544       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0204 06:43:43.578552       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0204 06:43:43.579922       1 shared_informer.go:320] Caches are synced for ephemeral
I0204 06:43:43.580042       1 shared_informer.go:320] Caches are synced for endpoint
I0204 06:43:43.580239       1 shared_informer.go:320] Caches are synced for service account
I0204 06:43:43.581437       1 shared_informer.go:320] Caches are synced for crt configmap
I0204 06:43:43.581462       1 shared_informer.go:320] Caches are synced for PV protection
I0204 06:43:43.581490       1 shared_informer.go:320] Caches are synced for persistent volume
I0204 06:43:43.583209       1 shared_informer.go:320] Caches are synced for HPA
I0204 06:43:43.584388       1 shared_informer.go:320] Caches are synced for resource quota
I0204 06:43:43.585466       1 shared_informer.go:320] Caches are synced for namespace
I0204 06:43:43.587882       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0204 06:43:43.589052       1 shared_informer.go:320] Caches are synced for daemon sets
I0204 06:43:43.590207       1 shared_informer.go:320] Caches are synced for TTL after finished
I0204 06:43:43.590273       1 shared_informer.go:320] Caches are synced for TTL
I0204 06:43:43.594443       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0204 06:43:43.596686       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0204 06:43:43.623095       1 shared_informer.go:320] Caches are synced for cronjob
I0204 06:43:43.624444       1 shared_informer.go:320] Caches are synced for disruption
I0204 06:43:43.625860       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0204 06:43:43.627971       1 shared_informer.go:320] Caches are synced for taint
I0204 06:43:43.628071       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0204 06:43:43.628239       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0204 06:43:43.628616       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0204 06:43:43.628915       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0204 06:43:43.628970       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0204 06:43:43.630179       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0204 06:43:43.631762       1 shared_informer.go:320] Caches are synced for ReplicationController
I0204 06:43:43.631799       1 shared_informer.go:320] Caches are synced for stateful set
I0204 06:43:43.633199       1 shared_informer.go:320] Caches are synced for job
I0204 06:43:43.634358       1 shared_informer.go:320] Caches are synced for expand
I0204 06:43:43.635546       1 shared_informer.go:320] Caches are synced for attach detach
I0204 06:43:43.637299       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0204 06:43:43.637536       1 shared_informer.go:320] Caches are synced for resource quota
I0204 06:43:43.641883       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0204 06:43:43.641925       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0204 06:43:43.641934       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0204 06:43:43.642129       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0204 06:43:43.653167       1 shared_informer.go:320] Caches are synced for garbage collector
I0204 06:43:44.533820       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0204 06:43:44.749033       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="1.063483601s"
I0204 06:43:44.760094       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="10.998532ms"
I0204 06:43:44.760186       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="31.877µs"
I0204 06:43:44.762327       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="48.866µs"
I0204 06:43:45.726653       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="48.176µs"
I0204 06:43:49.936933       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0204 06:44:14.268136       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="7.887148ms"
I0204 06:44:14.268215       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="48.268µs"
I0204 06:49:06.216368       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0204 06:51:49.003880       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0204 06:55:28.394889       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="17.361102ms"
I0204 06:55:28.400850       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="5.906833ms"
I0204 06:55:28.400945       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="29.79µs"
I0204 06:55:28.406916       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="101.988µs"
I0204 06:55:32.629814       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="30.79µs"
I0204 06:55:47.583500       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="50.937µs"
I0204 06:56:05.582768       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="56.134µs"
I0204 06:56:20.547748       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="86.76µs"
I0204 06:56:35.579746       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="42.348µs"
I0204 06:56:50.582531       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/static-web-deployment-c66958b95" duration="46.196µs"


==> kube-proxy [99ca2c9f9185] <==
I0204 06:43:45.176447       1 server_linux.go:66] "Using iptables proxy"
I0204 06:43:45.456492       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0204 06:43:45.456595       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0204 06:43:45.474461       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0204 06:43:45.474548       1 server_linux.go:170] "Using iptables Proxier"
I0204 06:43:45.476256       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0204 06:43:45.481911       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0204 06:43:45.488195       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0204 06:43:45.488328       1 server.go:497] "Version info" version="v1.32.0"
I0204 06:43:45.488368       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0204 06:43:45.494054       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0204 06:43:45.498909       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0204 06:43:45.499965       1 config.go:199] "Starting service config controller"
I0204 06:43:45.500004       1 shared_informer.go:313] Waiting for caches to sync for service config
I0204 06:43:45.500022       1 config.go:105] "Starting endpoint slice config controller"
I0204 06:43:45.500026       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0204 06:43:45.500508       1 config.go:329] "Starting node config controller"
I0204 06:43:45.500539       1 shared_informer.go:313] Waiting for caches to sync for node config
I0204 06:43:45.600576       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0204 06:43:45.600596       1 shared_informer.go:320] Caches are synced for service config
I0204 06:43:45.600618       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [002d93aa3ddf] <==
W0204 06:43:37.032907       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0204 06:43:37.032961       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0204 06:43:37.033141       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0204 06:43:37.092095       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0204 06:43:37.092137       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0204 06:43:37.094089       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0204 06:43:37.094149       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0204 06:43:37.094096       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0204 06:43:37.094529       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0204 06:43:37.172937       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0204 06:43:37.173030       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0204 06:43:37.173389       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0204 06:43:37.173482       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0204 06:43:37.173525       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0204 06:43:37.173548       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.173768       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0204 06:43:37.173894       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.173977       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0204 06:43:37.173989       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0204 06:43:37.174036       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0204 06:43:37.174056       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.174180       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0204 06:43:37.174301       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0204 06:43:37.174422       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0204 06:43:37.174439       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
W0204 06:43:37.174459       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0204 06:43:37.174475       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.174501       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0204 06:43:37.174306       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0204 06:43:37.174498       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0204 06:43:37.174544       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0204 06:43:37.174489       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0204 06:43:37.174428       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.174202       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0204 06:43:37.174667       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.174217       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0204 06:43:37.174833       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.174304       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0204 06:43:37.174346       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0204 06:43:37.175373       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0204 06:43:37.174874       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0204 06:43:37.991526       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0204 06:43:37.991575       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.025197       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0204 06:43:38.025250       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.027651       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0204 06:43:38.028016       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.075327       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0204 06:43:38.075372       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.077461       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0204 06:43:38.077505       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.113726       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0204 06:43:38.113770       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.151135       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0204 06:43:38.151168       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.300949       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0204 06:43:38.301000       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0204 06:43:38.550231       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0204 06:43:38.550283       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0204 06:43:41.495113       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732756    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732774    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-certs\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732820    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732882    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732902    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/d14ce008bee3a1f3bd7cf547688f9dfe-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"d14ce008bee3a1f3bd7cf547688f9dfe\") " pod="kube-system/kube-scheduler-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732918    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732929    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732944    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732957    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732969    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732979    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.732992    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.778665    2506 kubelet_node_status.go:76] "Attempting to register node" node="minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.792220    2506 kubelet_node_status.go:125] "Node was previously registered" node="minikube"
Feb 04 06:43:39 minikube kubelet[2506]: I0204 06:43:39.792319    2506 kubelet_node_status.go:79] "Successfully registered node" node="minikube"
Feb 04 06:43:40 minikube kubelet[2506]: I0204 06:43:40.518217    2506 apiserver.go:52] "Watching apiserver"
Feb 04 06:43:40 minikube kubelet[2506]: I0204 06:43:40.567849    2506 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Feb 04 06:43:40 minikube kubelet[2506]: I0204 06:43:40.669338    2506 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.669232167 podStartE2EDuration="1.669232167s" podCreationTimestamp="2025-02-04 06:43:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-04 06:43:40.669098861 +0000 UTC m=+1.214353708" watchObservedRunningTime="2025-02-04 06:43:40.669232167 +0000 UTC m=+1.214487009"
Feb 04 06:43:40 minikube kubelet[2506]: I0204 06:43:40.677202    2506 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Feb 04 06:43:40 minikube kubelet[2506]: E0204 06:43:40.690099    2506 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Feb 04 06:43:40 minikube kubelet[2506]: I0204 06:43:40.692668    2506 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.692643978 podStartE2EDuration="1.692643978s" podCreationTimestamp="2025-02-04 06:43:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-04 06:43:40.692607524 +0000 UTC m=+1.237862381" watchObservedRunningTime="2025-02-04 06:43:40.692643978 +0000 UTC m=+1.237898831"
Feb 04 06:43:40 minikube kubelet[2506]: I0204 06:43:40.786873    2506 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.786822285 podStartE2EDuration="1.786822285s" podCreationTimestamp="2025-02-04 06:43:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-04 06:43:40.703574065 +0000 UTC m=+1.248828912" watchObservedRunningTime="2025-02-04 06:43:40.786822285 +0000 UTC m=+1.332077155"
Feb 04 06:43:40 minikube kubelet[2506]: I0204 06:43:40.799833    2506 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.799776186 podStartE2EDuration="1.799776186s" podCreationTimestamp="2025-02-04 06:43:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-04 06:43:40.789499273 +0000 UTC m=+1.334754121" watchObservedRunningTime="2025-02-04 06:43:40.799776186 +0000 UTC m=+1.345031034"
Feb 04 06:43:43 minikube kubelet[2506]: I0204 06:43:43.687236    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/2db0edc3-3f42-4674-8314-67c299349d88-tmp\") pod \"storage-provisioner\" (UID: \"2db0edc3-3f42-4674-8314-67c299349d88\") " pod="kube-system/storage-provisioner"
Feb 04 06:43:43 minikube kubelet[2506]: I0204 06:43:43.687296    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xfpdr\" (UniqueName: \"kubernetes.io/projected/2db0edc3-3f42-4674-8314-67c299349d88-kube-api-access-xfpdr\") pod \"storage-provisioner\" (UID: \"2db0edc3-3f42-4674-8314-67c299349d88\") " pod="kube-system/storage-provisioner"
Feb 04 06:43:43 minikube kubelet[2506]: E0204 06:43:43.794162    2506 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 04 06:43:43 minikube kubelet[2506]: E0204 06:43:43.794203    2506 projected.go:194] Error preparing data for projected volume kube-api-access-xfpdr for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Feb 04 06:43:43 minikube kubelet[2506]: E0204 06:43:43.794279    2506 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/2db0edc3-3f42-4674-8314-67c299349d88-kube-api-access-xfpdr podName:2db0edc3-3f42-4674-8314-67c299349d88 nodeName:}" failed. No retries permitted until 2025-02-04 06:43:44.294258549 +0000 UTC m=+4.839513391 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-xfpdr" (UniqueName: "kubernetes.io/projected/2db0edc3-3f42-4674-8314-67c299349d88-kube-api-access-xfpdr") pod "storage-provisioner" (UID: "2db0edc3-3f42-4674-8314-67c299349d88") : configmap "kube-root-ca.crt" not found
Feb 04 06:43:44 minikube kubelet[2506]: E0204 06:43:44.392758    2506 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 04 06:43:44 minikube kubelet[2506]: E0204 06:43:44.392811    2506 projected.go:194] Error preparing data for projected volume kube-api-access-xfpdr for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Feb 04 06:43:44 minikube kubelet[2506]: E0204 06:43:44.392878    2506 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/2db0edc3-3f42-4674-8314-67c299349d88-kube-api-access-xfpdr podName:2db0edc3-3f42-4674-8314-67c299349d88 nodeName:}" failed. No retries permitted until 2025-02-04 06:43:45.392858274 +0000 UTC m=+5.938113122 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-xfpdr" (UniqueName: "kubernetes.io/projected/2db0edc3-3f42-4674-8314-67c299349d88-kube-api-access-xfpdr") pod "storage-provisioner" (UID: "2db0edc3-3f42-4674-8314-67c299349d88") : configmap "kube-root-ca.crt" not found
Feb 04 06:43:44 minikube kubelet[2506]: I0204 06:43:44.593898    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-j7q5h\" (UniqueName: \"kubernetes.io/projected/ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a-kube-api-access-j7q5h\") pod \"kube-proxy-99sk6\" (UID: \"ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a\") " pod="kube-system/kube-proxy-99sk6"
Feb 04 06:43:44 minikube kubelet[2506]: I0204 06:43:44.593965    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a-lib-modules\") pod \"kube-proxy-99sk6\" (UID: \"ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a\") " pod="kube-system/kube-proxy-99sk6"
Feb 04 06:43:44 minikube kubelet[2506]: I0204 06:43:44.593978    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a-xtables-lock\") pod \"kube-proxy-99sk6\" (UID: \"ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a\") " pod="kube-system/kube-proxy-99sk6"
Feb 04 06:43:44 minikube kubelet[2506]: I0204 06:43:44.593989    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a-kube-proxy\") pod \"kube-proxy-99sk6\" (UID: \"ed6b0dc7-70f8-4c00-9ae3-22b7329b8b8a\") " pod="kube-system/kube-proxy-99sk6"
Feb 04 06:43:44 minikube kubelet[2506]: I0204 06:43:44.795411    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/a96fad5e-24f8-46f6-ba23-0e5929514582-config-volume\") pod \"coredns-668d6bf9bc-s2745\" (UID: \"a96fad5e-24f8-46f6-ba23-0e5929514582\") " pod="kube-system/coredns-668d6bf9bc-s2745"
Feb 04 06:43:44 minikube kubelet[2506]: I0204 06:43:44.795478    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-m9scq\" (UniqueName: \"kubernetes.io/projected/a96fad5e-24f8-46f6-ba23-0e5929514582-kube-api-access-m9scq\") pod \"coredns-668d6bf9bc-s2745\" (UID: \"a96fad5e-24f8-46f6-ba23-0e5929514582\") " pod="kube-system/coredns-668d6bf9bc-s2745"
Feb 04 06:43:45 minikube kubelet[2506]: I0204 06:43:45.713367    2506 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=5.713344544 podStartE2EDuration="5.713344544s" podCreationTimestamp="2025-02-04 06:43:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-04 06:43:45.713289683 +0000 UTC m=+6.258544531" watchObservedRunningTime="2025-02-04 06:43:45.713344544 +0000 UTC m=+6.258599385"
Feb 04 06:43:45 minikube kubelet[2506]: I0204 06:43:45.774694    2506 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-99sk6" podStartSLOduration=1.774679041 podStartE2EDuration="1.774679041s" podCreationTimestamp="2025-02-04 06:43:44 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-04 06:43:45.774417273 +0000 UTC m=+6.319672118" watchObservedRunningTime="2025-02-04 06:43:45.774679041 +0000 UTC m=+6.319933884"
Feb 04 06:43:45 minikube kubelet[2506]: I0204 06:43:45.774776    2506 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-s2745" podStartSLOduration=1.774771534 podStartE2EDuration="1.774771534s" podCreationTimestamp="2025-02-04 06:43:44 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-04 06:43:45.726571412 +0000 UTC m=+6.271826255" watchObservedRunningTime="2025-02-04 06:43:45.774771534 +0000 UTC m=+6.320026376"
Feb 04 06:43:49 minikube kubelet[2506]: I0204 06:43:49.868765    2506 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Feb 04 06:43:49 minikube kubelet[2506]: I0204 06:43:49.870301    2506 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Feb 04 06:43:54 minikube kubelet[2506]: I0204 06:43:54.174757    2506 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 04 06:55:28 minikube kubelet[2506]: I0204 06:55:28.440702    2506 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-47zkl\" (UniqueName: \"kubernetes.io/projected/0192a08f-4a79-4b6c-86e1-11bbe9a63a87-kube-api-access-47zkl\") pod \"static-web-deployment-c66958b95-bxkmg\" (UID: \"0192a08f-4a79-4b6c-86e1-11bbe9a63a87\") " pod="default/static-web-deployment-c66958b95-bxkmg"
Feb 04 06:55:32 minikube kubelet[2506]: E0204 06:55:32.246695    2506 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="static-web-project:latest"
Feb 04 06:55:32 minikube kubelet[2506]: E0204 06:55:32.246782    2506 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="static-web-project:latest"
Feb 04 06:55:32 minikube kubelet[2506]: E0204 06:55:32.246904    2506 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:static-web,Image:static-web-project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47zkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod static-web-deployment-c66958b95-bxkmg_default(0192a08f-4a79-4b6c-86e1-11bbe9a63a87): ErrImagePull: Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 04 06:55:32 minikube kubelet[2506]: E0204 06:55:32.247997    2506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"static-web\" with ErrImagePull: \"Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/static-web-deployment-c66958b95-bxkmg" podUID="0192a08f-4a79-4b6c-86e1-11bbe9a63a87"
Feb 04 06:55:32 minikube kubelet[2506]: E0204 06:55:32.589328    2506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"static-web\" with ImagePullBackOff: \"Back-off pulling image \\\"static-web-project\\\": ErrImagePull: Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/static-web-deployment-c66958b95-bxkmg" podUID="0192a08f-4a79-4b6c-86e1-11bbe9a63a87"
Feb 04 06:55:50 minikube kubelet[2506]: E0204 06:55:50.771749    2506 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="static-web-project:latest"
Feb 04 06:55:50 minikube kubelet[2506]: E0204 06:55:50.771829    2506 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="static-web-project:latest"
Feb 04 06:55:50 minikube kubelet[2506]: E0204 06:55:50.771931    2506 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:static-web,Image:static-web-project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47zkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod static-web-deployment-c66958b95-bxkmg_default(0192a08f-4a79-4b6c-86e1-11bbe9a63a87): ErrImagePull: Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 04 06:55:50 minikube kubelet[2506]: E0204 06:55:50.773138    2506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"static-web\" with ErrImagePull: \"Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/static-web-deployment-c66958b95-bxkmg" podUID="0192a08f-4a79-4b6c-86e1-11bbe9a63a87"
Feb 04 06:56:05 minikube kubelet[2506]: E0204 06:56:05.538573    2506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"static-web\" with ImagePullBackOff: \"Back-off pulling image \\\"static-web-project\\\": ErrImagePull: Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/static-web-deployment-c66958b95-bxkmg" podUID="0192a08f-4a79-4b6c-86e1-11bbe9a63a87"
Feb 04 06:56:24 minikube kubelet[2506]: E0204 06:56:24.292442    2506 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="static-web-project:latest"
Feb 04 06:56:24 minikube kubelet[2506]: E0204 06:56:24.292516    2506 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="static-web-project:latest"
Feb 04 06:56:24 minikube kubelet[2506]: E0204 06:56:24.292625    2506 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:static-web,Image:static-web-project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47zkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod static-web-deployment-c66958b95-bxkmg_default(0192a08f-4a79-4b6c-86e1-11bbe9a63a87): ErrImagePull: Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 04 06:56:24 minikube kubelet[2506]: E0204 06:56:24.293908    2506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"static-web\" with ErrImagePull: \"Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/static-web-deployment-c66958b95-bxkmg" podUID="0192a08f-4a79-4b6c-86e1-11bbe9a63a87"
Feb 04 06:56:35 minikube kubelet[2506]: E0204 06:56:35.536587    2506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"static-web\" with ImagePullBackOff: \"Back-off pulling image \\\"static-web-project\\\": ErrImagePull: Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/static-web-deployment-c66958b95-bxkmg" podUID="0192a08f-4a79-4b6c-86e1-11bbe9a63a87"
Feb 04 06:56:50 minikube kubelet[2506]: E0204 06:56:50.537167    2506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"static-web\" with ImagePullBackOff: \"Back-off pulling image \\\"static-web-project\\\": ErrImagePull: Error response from daemon: pull access denied for static-web-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/static-web-deployment-c66958b95-bxkmg" podUID="0192a08f-4a79-4b6c-86e1-11bbe9a63a87"


==> storage-provisioner [a3266c33711e] <==
I0204 06:43:45.697978       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0204 06:43:45.706076       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0204 06:43:45.706140       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0204 06:43:45.714543       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0204 06:43:45.714711       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f50ec490-2d14-4b9a-afa4-13685c8b2adc!
I0204 06:43:45.714841       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"2079781d-bf5e-40a7-a643-dbf7f8d3740b", APIVersion:"v1", ResourceVersion:"358", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f50ec490-2d14-4b9a-afa4-13685c8b2adc became leader
I0204 06:43:45.814955       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f50ec490-2d14-4b9a-afa4-13685c8b2adc!

